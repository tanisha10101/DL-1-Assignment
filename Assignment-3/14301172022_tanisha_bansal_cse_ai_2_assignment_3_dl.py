# -*- coding: utf-8 -*-
"""14301172022_TANISHA_BANSAL_CSE-AI-2_ASSIGNMENT-3_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S9k56b3r60OrSyYjWygKEpI5olSo6GlP

**SUBMITTED BY:-**

**NAME: TANISHA BANSAL**

**BATCH: CSE-AI - 2 (2026)**

**ROLL NO: 14301172022**

**ASSIGNMENT - 3 (DEEP LEARNING)**

**Q. Write a program to implement Artificial Neural Network for MNIST dataset.**
"""

pip install tensorflow matplotlib

# Importing the libraries
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# Loading MNIST dataset
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalizing the data (pixel values are between 0 and 255, so we will scale them between 0 and 1)
X_train = X_train / 255.0
X_test = X_test / 255.0

# Reshaping data to fit the input of the neural network
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)

# Building the Artificial Neural Network (ANN)
model = models.Sequential()

# Now, we will Input layer (and Flatten the 28x28 images into a 1D vector of 784 values)
model.add(layers.Flatten(input_shape=(28, 28, 1)))

# This will be our Hidden layer with 128 neurons and ReLU activation function
model.add(layers.Dense(128, activation='relu'))

# This will be our Output layer with 10 neurons (one for each digit) and softmax activation
model.add(layers.Dense(10, activation='softmax'))

# Now, we will be Compiling this model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Now, we will train this model
history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))

# Evaluating the model on the test data
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'\nTest accuracy: {test_acc}')

# Now, we are going to plot the accuracy and loss
def plot_history(history):
    # Plotting training & validation accuracy values
    plt.figure(figsize=(12, 4))

    # Accuracy plot
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend(['Train', 'Test'], loc='upper left')

    # Loss plot
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(['Train', 'Test'], loc='upper left')

    plt.show()

# Plot training history
plot_history(history)

# we can also perform data augmentation
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1
)

datagen.fit(X_train)

# now, during training, we will replace the model.fit with
model.fit(datagen.flow(X_train, y_train, batch_size=32),
          epochs=5,
          validation_data=(X_test, y_test))

# we can also perform model regulation
# Adding a dropout layer to the model
model.add(layers.Dropout(0.2))
# Dropout 20% of the neurons after the hidden layer

# Adding L2 regularization to the dense layers
from tensorflow.keras import regularizers
model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))

# we can also normalize it
model.add(layers.BatchNormalization())